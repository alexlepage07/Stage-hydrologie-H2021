\documentclass[11pt,letterpaper]{article}
\author{Alexandre Lepage}
\usepackage[left=2.00cm, right=2.00cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[frenchb]{babel} % Reconnaître les caractères francophones.
\usepackage[procnames]{listings}
\usepackage[ruled,vlined, linesnumbered]{algorithm2e} % Faire de jolies algorithmes comme dans les cours d'IFT.
\usepackage[squaren]{SIunits}
\usepackage{amsfonts}
\usepackage{amsmath} % pour utiliser des maths de base 
\usepackage{amssymb} % pour faire \mathcal{}=>des lettres ''cursives''
\usepackage{amsthm} % La petite boîte de fin de preuve
\usepackage{array}
\usepackage{bbm}
\usepackage{booktabs}
\usepackage{braket}
\usepackage{breakcites} % Faire en sorte que les citations ne sortent pas dans la marge
\usepackage{caption}
\usepackage{color}
\usepackage{comment}
\usepackage{diagbox} % diagonale dans les tableaux
\usepackage{dsfont} % Faire des belles indicatrices                         
\usepackage{enumitem}
\usepackage{enumitem} % Permet d'avoir plus de flexibilité dans les enumerations.
\usepackage{epsfig}
\usepackage{etoolbox} % Ajouter plus d'espace entre les éléments de l'environnement "cases"
\usepackage{fancyvrb} % Les varbatims gardent l'indentation
\usepackage{float} % placer les tableaux et images où tu veux
\usepackage{graphicx} % Insérer des graphiques
\usepackage{graphicx} % pour importer des images...http://www.tex.ac.uk/cgi-bin/texfaq2html?label=figurehere
\usepackage{hyperref} % Faire des hyperliens
\usepackage{lipsum}
\usepackage{listings}
\usepackage{mathrsfs} % Faire le symbole de la transformée de Laplace
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{multirow} % Fusionner des lignes dans un tableau
\usepackage{pgfplots}
\usepackage{pst-node}
\usepackage{setspace} % Ajouter plus d'espace entre les éléments de l'environnement "cases"
\usepackage{soul} % Surligner des passages mathématiques
\usepackage{subcaption} % Avoir plusieurs sous-figures (graphiques) dans une figures et pouvoire les étiqueter
\usepackage{titlesec} % automatique, pour faire des sous-titres moins laids
\usepackage{verbatim} % Inclure un fichier .text en verbatim
\usepackage{wasysym} 
\usepackage{wrapfig} % Permet d'intégrer des graphiques à travers du texte.
\usepackage{xcolor}


\pgfplotsset{width=10cm, compat=1.9}

% Changer la couleur des hyperliens
\hypersetup{colorlinks = true,
	allcolors  = blue, % default color = black
	%	citecolor  = black
}  


% redefine \VerbatimInput
\RecustomVerbatimCommand{\VerbatimInput}{VerbatimInput}%
{fontsize=\footnotesize,
	%
	frame=lines,  % top and bottom rule only
	framesep=2em, % separation between frame and text
	rulecolor=\color{Gray},
	%
	label=\fbox{\color{Black}data.txt},
	labelposition=topline,
	%
	commandchars=\|\(\), % escape character and argument delimiters for
	% commands within the verbatim
	commentchar=*        % comment character
}


\newtheorem{lemme}{Lemme}
\newtheorem{preuve}{Preuve}
\newtheorem{code}{Code informatique}
\newtheorem{exemple}{Exemple}
\newtheorem{scenario}{Scénario}
\newtheorem{algo}{Algorithme}
\newtheorem{definition}{Définition}
\newtheorem{proposition}{Proposition}
\newtheorem{propriete}{Propriété}
\newtheorem{remarque}{Remarque}
\newtheorem{theorem}{Théorème}
\newtheorem{corollaire}{Corollaire}


\begin{document}
	\renewcommand{\tablename}{Tableau}
	\renewcommand{\figurename}{Illustration}
	\renewcommand{\labelitemi}{$\bullet$}
	\renewenvironment{proof}{\noindent{\bfseries Preuve.}}{\qed\\}
	\renewcommand{\natural}{\mathbb{N}}
	\newcommand{\reel}{\mathbb{R}}
	\newcommand{\E}{\mathbb{E}}
	\newcommand{\card}{\mathrm{card}}
	\renewcommand{\P}{\mathbb{\mathbb{P}}}
	
	\begin{titlepage}
		\centering % Centre everything on the title page
		
		\scshape % Use small caps for all text on the title page
		
		\vspace*{4\baselineskip} % White space at the top of the page
		
		%------------------------------------------------
		%	Title
		%------------------------------------------------
				
		\rule{\textwidth}{1.6pt}\vspace*{-\baselineskip}\vspace*{2pt} % Thick horizontal rule
		\rule{\textwidth}{0.4pt} % Thin horizontal rule
		
		\vspace{0.75\baselineskip} % Whitespace above the title
		{\LARGE Rapport de stage\\} % Title
		\vspace{0.75\baselineskip}
		{\large Stage à titre de chercheur au sein de \\
			l'Institut nationale de recherche scientifique\\} % Title
		\vspace{0.75\baselineskip} % Whitespace below the title
		
		\rule{\textwidth}{0.4pt}\vspace*{-\baselineskip}\vspace{3.2pt} % Thin horizontal rule
		\rule{\textwidth}{1.6pt} % Thick horizontal rule
		
		\vspace{2\baselineskip} % Whitespace after the title block
		
		%------------------------------------------------
		%	Subtitle
		%------------------------------------------------
		
		Travail présenté à \\
		{\scshape\large M. Ilie Radu Mitric\\}
		
		\vspace*{2\baselineskip}
		
		Dans le cadre du cours\\
		{\scshape\large Travail actuariel pratique en entreprise \\ ACT-7005 }
		
		% Subtitle or further description
		
		\vspace*{2\baselineskip} % Whitespace under the subtitle
		
		%------------------------------------------------
		%	Editor(s)
		%------------------------------------------------
		
		Préparé par\\
		{\scshape\large Alexandre Lepage,\\ 111 144 776} 
		% Editor list
		\vspace*{1\baselineskip}
		
		Supervisé par le professeur\\
		{\scshape\large Fateh Chebana, PhD} % Editor list
		
		\vspace*{2\baselineskip}
		
		le \today
		
		\vspace{0.5\baselineskip} % Whitespace below the editor list
		
		\vfill % Whitespace between editor names and publisher logo
		
		%------------------------------------------------
		%	Publisher
		%------------------------------------------------
		
		\includegraphics[height=1.2cm]{UL_P.pdf}\\
		
		Faculté des sciences et de génie\\
		École d'actuariat\\
		Université Laval\\     
	\end{titlepage}
	
		\pagenumbering{Roman} % Pagination en chiffres romains
		\setcounter{page}{0} 
		
		\newpage
		\strut % Page blanche
		\newpage
		
		\tableofcontents
		\newpage
		
    %	\renewcommand{\listfigurename}{Liste des illustrations}
	%	\listoffigures
	%	\listoftables
	%	\newpage
		
		\pagenumbering{arabic} % Pagination en chiffres normaux
		\setcounter{page}{1}
	
	
	\section{Sommaire}
	
	\section{Description de l'entreprise}
	L’Institut national de la recherche scientifique (INRS) est un centre de recherche universitaire composé 
	de quatre établissements répartis dans les grands centres du Québec. Il est composé des établissements de recherche et de formation thématiques Armand-Frappier Santé Biotechnologie (Laval), Eau Terre Environnement (Québec), Énergie Matériaux Télécommunications (Montréal et Varennes) et Urbanisation Culture Société (Montréal et Québec).
	
	Comme le superviseur de ce stage, le professeur Fateh Chebana\footnote{\url{https://inrs.ca/la-recherche/professeurs/fateh-chebana/}}, appartient au centre Eau Terre Environnement\footnote{\url{https://inrs.ca/linrs/centres-de-recherche/centre-eau-terre-environnement/}},c'est sur ce dernier que l'accent est porté. 
	Ce centre de recherche est situé à Québec, au 490, rue de la Couronne.
	Il compte près de 36 professeurs répartis dans 6 programmes d'études dont les seuls en science de l'eau au Québec.\\
	
	La mission de l'INRS est de promouvoir la recherche fondamentale et appliquée. Il valorise les études supérieures et la formation des chercheurs de demain. Dans l'atteinte de son objectif, l'institut doit orienter ses activités vers le développement économique, social et culturel du Québec, tout en assurant le transfert des connaissances et des technologies dans l'ensemble des secteurs où il œuvre.
	
	Plus spécifiquement, le Centre Eau Terre Environnement a pour objectif d'\textbf{améliorer la protection, la conservation et la mise en valeur des ressources naturelles}. Les activités de recherche, quant à elles, sont concentrées dans quatre thématiques de recherche prioritaires : l’assainissement et la valorisation des résidus, la biogéochimie aquatique, l’hydrologie et les sciences de la Terre. Pour des exemples récents d'études réalisés par ce centre, on peut consulter le lien suivant: \url{https://inrs.ca/wp-content/uploads/2021/01/4-Fiche-Hydrologie_ETE-2019-20.pdf}.\\
	
	Les valeurs véhiculées par cette institution sont l'excellence, l'interdisciplinarité, l'engagement, l'équité et l'intégrité.
	%
	Autant en recherche que lors de la formation de nouveaux chercheurs, l'Institut cherche à tendre vers l'\textbf{excellence}.
	Pour ce faire, elle attend de ses membres qu'ils soient \textbf{intègres} dans leur travaille puisque cette valeur est un pilier de la recherche scientifique. Elle attend également de leur part qu'ils démontrent de l’\textbf{engagement} envers la mission de l’INRS. 
	%
	Avec ses quatre centres et ses nombreuses chaires de recherche\footnote{\url{https://inrs.ca/la-recherche/chaires-groupes-et-reseaux-de-recherche/}} dans des domaines tous plus variés les uns que les autres, l'INRS se définit à travers l'\textbf{interdisciplinarité}. C'est en unissant les forces de tous et chacun que l'on trouve des solutions durables et innovantes. 
	%
	Finalement, l'INRS propose un plan d'action\footnote{\url{https://inrs.ca/wp-content/uploads/2020/12/Plan_action_EDI_CRC_INRS_2020_12_07.pdf}} en matière d'\textbf{équité}, de diversité et d'inclusion afin de valoriser la représentation des personnes des quatre groupes désignés (femmes, personnes en situation de handicap, autochtones, membres de minorités visibles) dans l’attribution de ces chaires.
	
	
	\section{Attentes de l'étudiant}\label{sect_attentes_etudiant}
		Ce stage fait suite aux travaux réalisés lors de l'été 2020 sous la supervision des professeurs Étienne Marceau et Hélène Cossette. L'objet de ces travaux était de faire avancer la recherche actuarielle sur l'introduction d'une structure de dépendance dans les modèles de processus de renouvellement avec récompenses tels que définis par \cite{andersen1957collective} et décrit dans \cite{grimmett2001probability} et \cite{gallager2013stochastic}. À la conclusion du mandat de l'été 2020, l'étape suivante était de trouver une application au modèle proposé. Comme plusieurs articles sur les copules et la modélisation des risques extrêmes sont en lien avec des contextes de climatologie ou, plus spécifiquement, en hydrologie (Voir p. ex. \cite{salvadori2006statistical}, \cite{salvadori2007use} et \cite{vandenberghe2010fitting}), ce domaine a suscité mon intérêt. Comme Mme Cossette et M. Marceau ont eu des étudiants qui ont fait des stages avec M. Chebana dans le passé et que ceux-ci se sont bien conclu, ce dernier m'a été référé pour travailler sur un article collaboratif. L'idée étant que nous apportions le sujet et que celui-ci nous aidait à travers son expertise à trouver une application adéquate et à trouver des références intéressantes pour guider les recherche.\\
		
		Pour aller plus en profondeur dans mes attentes, je souhaitais avoir beaucoup d'autonomie dans mes démarches. Malgré tout, une rencontre par semaine permet de tenir les collaborateurs au courant des avancements, de suggérer des lectures pertinentes, de partager des idées et de s'assurer que le projet va dans la direction que tout le monde s'était fixé. Pour cette raison, cette rencontre doit durer un temps suffisant et les différents collaborateurs doivent démontrer un intérêt commun pour le projet et s'impliquer.
		
%		Un dernier point important était le financement du projet. Considérant que M. Chebana désirait voir son nom apparaître sur l'article résultant de ce travail, il était naturel qu'il contribue au financement des travaux de l'étudiant par une bourse de recherche.
	
	\section{Attentes du superviseur}
		
	
	
	\section{Mandat}\label{sect_mandat}
	Comme il est mentionné dans la section \ref{sect_attentes_etudiant}, ce stage fait suite aux travaux réalisés lors de l'été 2020. L'idée était de trouver une application des processus de renouvellements avec récompenses lorsqu'il existe un lien de dépendance entre les temps inter-occurrences et la récompense (bonus ou malus, selon le contexte). Les premières semaines du stage ont donc été consacrées à trouver cette application. Celle-ci c'est présentée à la suite d'une présentation par M. Christian Genest\footnote{\url{https://www.math.mcgill.ca/cgenest/}}, professeur à l'université McGill, le 28 janvier 2021. Lors de cette présentation, M. Genest a présenté les résultats d'un article qu'il a publié en 2019, soit \cite{jalbert2019modelling}. Le travail réalisé lors de ce stage reprend donc le contexte et s'inspire du modèle de cet article pour introduire le modèle développé lors de l'été 2020 dans la littérature en hydrologie.\\
	
	La description du mandat est découpé comme suit: D'abord, le sujet de l'étude est introduit dans la sous-section \ref{sect_Intro_modele}. Puis, le modèle étudié est décrit de façon détaillé dans la section \ref{sect_model}. Les résultats obtenus pour les deux études de cas réalisés sont présentés dans la sous-section \ref{sect_resultats}. Finalement, la description du mandat est conclue avec la sous-section \ref{sect_apprentissages} les apprentissages réalisés dans le cadre de ce travail sont décrits.
	
	
		\subsection{Introduction au modèle étudié}\label{sect_Intro_modele}
		Autant en assurance qu'en hydrologie, le sujet des inondations suscite l'intérêt des chercheurs qui tentent de modéliser ces événements afin de mieux se préparer à d'éventuelles catastrophes. Entre autres, le débordement du lac Champlain en 2011 a suscité l'intérêt de \cite{riboust2016analysis} qui a cherché à connaître les éléments déclencheurs d'une telle catastrophe. Lors de son étude, il arriva à la conclusion que, bien que la fonte des neiges soit une variable explicative importante, c'est l'accumulation de précipitations extrêmes dans la période de mars à juin qui a été la cause principale du désastre.
		
		Suite à la publication de \cite{riboust2016analysis}, \cite{jalbert2019modelling} a cherché à prédire l'accumulation des pluies printanières du lac Champlain afin d'estimer l'amplitude maximale qu'un tel événement aurait pu avoir et de calculer l'espérance du temps qui s'écoulera avant qu'un incident d'une telle ampleur survienne à nouveau. 
		%
		Le modèle ainsi conçu sépare la modélisation des pluies en deux composantes. Une portion régulière qui représente la quantité de pluie totale tombée selon les normes saisonnières et une portion extrême où on considère les jours où la quantité de pluie tombée dépasse un certain seuil. Tandis que la première est bien modélisée avec une loi normale, la seconde nécessite plus de travail.
		%
		En effet, afin de modéliser la quantité quotidienne de pluie tombée dans les cas extrêmes, \cite{jalbert2019modelling} utilise la loi Pareto généralisée, aussi connue sous l'appellation \textit{Peaks-Over-Threshold}(POT). Cependant, l'estimateur du paramètre de forme de la loi POT calculé sur les valeurs quotidiennes faisaient en sorte que la distribution avait un support fini. Se faisant,  la probabilité qu'un événement de l'envergure de 2011 se produise était quasiment nulle. Conséquemment, \cite{jalbert2019modelling} proposa une extension du modèle POT où il divise la quantité de pluie tombée lors d'une journée de pluie extrême par la proportion que cette quantité représente sur l'ensemble des précipitations tombées au cours d'une période de pluie continue. Grâce à ce stratagème, le modèle ainsi développé n'a plus de problème de distribution borné pour modéliser la sévérité des précipitations extrêmes.\\
		
		Le modèle que nous proposons est une alternative à celui de \cite{jalbert2019modelling}. Il reprend le concept de regroupement des jours en périodes de pluie continue, mais il adopte une approche légèrement plus intuitive en s'inspirant grandement de la méthodologie proposée dans \cite{zhang2019copulas}, \cite{salvadori2006statistical} et \cite{shaw2010hydrology}. 
		%			
		Comme il est expliqué, entre autre, au chapitre 9.6 de \cite{shaw2010hydrology}, les modèles de précipitations utilisés dans la pratique, communément appelés modèles DDF (Dept, Duration, Frequency), sous-tendent les variables de sévérité de durée et de fréquence, lesquelles ont toutes leur importance. Or, \cite{jalbert2019modelling} se concentre principalement sur la sévérité. La fréquence est modélisée avec une loi de Poisson et l'aspect de la durée des événements n'est pas pris en compte.
		%
		Notre approche, quant à elle, s'intéresse à toutes ces variables aléatoires et aux relations qui les unissent. Nous incorporons des notions des processus de renouvellement avec récompenses aux concepts communément utilisés en hydrologie afin d'améliorer la précision du modèle de fréquence et les liens de dépendance sont modélisés à l'aide de la théorie des copules introduite par \cite{joe1997multivariate}. Également, une attention particulière est apportée à la non-stationnarité des distributions de probabilité afin de prendre en compte le contexte des changements climatiques.
		Un modèle similaire est utilisé dans la modélisation des tempêtes par \cite{salvadori2006statistical} et \cite{salvadori2007use}.
		
		
		\subsection{Le modèle proposé}\label{sect_model}
		L'intuition derrière le modèle proposé est de considérer les périodes de précipitation continue comme des événements uniques en agrégeant la quantité de pluie tombée lors de celles-ci. Se faisant, non seulement on considère les jours où le volume d'eau tombé sort de l'ordinaire, mais on considère également les séquences où le nombre de jours de pluie continue est anormalement élevé. 
		%
		Afin de modéliser la fréquence des événements extrêmes, les processus de renouvellement alternés tels que présentés dans \cite{small1986relationship} et \cite{salvadori2006statistical}) sont utilisés. Cette approche permet d'observer une structure de dépendance non négligeable entre la sévérité, la durée et le temps écoulé depuis le dernier événement extrême. Cette dépendance est modélisée à l'aide d'une copule en vigne (voir \cite{joe2010regular}). Puis, pour faire suite à \cite{jalbert2019modelling} la théorie des valeurs extrêmes (voir \cite{hosking1987parameter}) est utilisée pour modéliser la sévérité des précipitations extrêmes avec la loi de Pareto généralisée (GP).
		
		\subsubsection{Description du modèle}\label{subsect_description_modele}
		Pour une année $m, m\geq 0$, on définit la suite de v.a. $\underline{Y}^{(m)} = \left\lbrace Y_l^{(m)}, l\in \{1,\dots,91\}\right\rbrace$, où $Y_l^{(m)}$ représente la quantité, en mm de pluie, tombée lors des 91 jours du printemps, c.-à-d. du $1^{\mathrm{er}}$ avril au 30 juin. Selon \cite{riboust2016analysis} et \cite{jalbert2019modelling}, cette période de temps est la plus critique lors des crues printanières. 
		
		Soit $\underline{\mathcal{C}}^{(m)} = \left\lbrace \mathcal{C}_j^{(m)}, j\in \{1,\dots, n_\mathcal{C}^{(m)}\}\right\rbrace$, la suite des $n_\mathcal{C}^{(m)}$ clusters regroupant les jours de pluie continue pour l'année $m$. En concordance avec \cite{jalbert2019modelling}, on définit un cluster comme une séquence de jours de pluie suivie par au moins une journée d'ensoleillement (0 mm de pluie).
		%
		On définit la suite de v.a. $\underline{K}^{(m)} = \left\lbrace K_j^{(m)}, j\in \{1,\dots, n_{\mathcal{C}}^{(m)}\}\right\rbrace$, où $K_j^{(m)}$ représente la quantité de pluie totale tombée lors de la période $\mathcal{C}_j^{(m)}$ telle que $K_j^{(m)} = \sum_{l\in \mathcal{C}_j^{(m)}} Y_l^{(m)},\ j=1,\dots,n_\mathcal{C}^{(m)},\ \forall m$.
		
		\begin{exemple}\label{ex_clustering}
			Supposons que, pour une année $m$, on ait $Y_{1}^{(m)}=30,\ Y_{2}^{(m)}=0,\ Y_{3}^{(m)}=2,\ Y_{4}^{(m)}=1,\ Y_{5}^{(m)}=3,\ Y_{6}^{(m)}=0,\ Y_{7}^{(m)}=0,\ Y_{8}^{(m)}=18,\ Y_{9}^{(m)}=3,\ Y_{10}^{(m)}=0$. 
			Alors $\mathcal{C}_1 = \{1\},\ \mathcal{C}_2=\{3,4,5\},\ \mathcal{C}_3=\{8,9\}$. 
			De plus, $K_1^{(m)} = Y_1^{(m)} = 30,\ K_2^{(m)}=Y_3^{(m)} + Y_4^{(m)} + Y_5^{(m)} = 6,\ K_3^{(m)}=Y_8^{(m)} + Y_9^{(m)} = 21$.
		\end{exemple}
		
		Pour une année $m$ et pour un seuil critique $u$, on sépare les événements en deux sous-ensembles
		$\mathcal{Z}^{(m)} = \lbrace j: K_j^{(m)} < u \rbrace$ et 
		$\mathcal{X}^{(m)} = \lbrace j: K_j^{(m)} \geq u \rbrace.$
		%
		On définira alors
		$\underline{X}^{(m)} =\left\lbrace X_i^{(m)}, i\in  \{1,\dots,\mathrm{card}(\mathcal{X}^{(m)})\}\right\rbrace = \{K_j^{(m)}: j \in \mathcal{X}^{(m)}\}$ comme étant une suite de v.a. où $X_i^{(m)}$ représente la quantité de pluie tombée lors du $i$-ème cluster extrême de l'année $m$.
		
		\paragraph{Exemple \ref{ex_clustering}, suite.} 
		\textit{On fixe un seuil $u=18$. L'ensemble $\mathcal{X}^{(m)}$ correspond à $j\in\{1,3\}$. On a donc $X_1^{(m)}=K_1^{(m)}=30$, et $X_2^{(m)} = K_3^{(m)} = 21$. Puis, pour ce qui est des événements non-extrêmes, l'ensemble $\mathcal{Z}^{(m)}$ correspond au complément de l'ensemble $\mathcal{X}^{(m)}$. On a alors $\mathcal{Z}^{(m)} = \{2\}.$
		}\\
		
		Soit $\underline{T}^{(m)} = \left\lbrace T_i^{(m)}, i\in\{0,1,\dots, \mathrm{card}(\mathcal{X}^{(m)})\}\right\rbrace$ une suite de v.a. où $T_i^{(m)}$ représente la première journée du cluster $\{\mathcal{C}_j^{(m)}, j\in\mathcal{X}^{(m)}\}$ associé à $X_i^{(m)}$. On défini que $T_0^{(m)}$ correspond au $1^{\mathrm{er}}$ avril de l'année $m$. Afin de calculer $T_i^{(m)}\ \forall\ i,m$, on définit une date d'origine qui correspond au jour zéro (p.ex. $T_0=1^{\mathrm{er}}$ avril 1900), puis, on converti cette date en format numérique selon le nombre de jours écoulé depuis cette date de référence. Cette opération se réalise automatiquement en enchaînant les fonctions \texttt{as.Date} avec \texttt{as.numeric} dans le langage de programmation \texttt{R}.
		
		Soit $\underline{D}^{(m)}= \left\lbrace D_i^{(m)}, i\in  \{1,\dots,\mathrm{card}(\mathcal{X}^{(m)})\}\right\rbrace$, une suite de v.a. où $D_i^{(m)}$ représente la durée, en nombre de jours, du $i$-ème événement extrême de l'année $m$. Autrement dit, cela signifie que $\{D_i^{(m)}, i\in\{1,\dots,\mathrm{card}(\mathcal{X}^{(m)})\} = \{\mathrm{card}(C_j^{(m)}): j\in\mathcal{X}^{(m)}\}$.
		
		Soit $\underline{W}^{(m)} = \left\lbrace W_i^{(m)}, i\in  \{1,\dots,\mathrm{card}(\mathcal{X}^{(m)})\}\right\rbrace$, une suite de v.a. où $W_i^{(m)}$ représente le temps écoulé depuis la fin du dernier événement extrême telle que 
		$$W_i^{(m)} := T_i^{(m)} - T_{i-1}^{(m)} - D_{i-1}^{(m)} = T_{i}^{(m)} - T_{i-1}^{\star(m)},$$ 
		où $T_i^{\star(m)} := T_0^{(m)} + \sum_{k=0}^{i} W_k^{(m)} + D_k^{(m)}$. Par convention, on a $W_0^{(m)} = D_0^{(m)} = 0,\ \forall m$.
		
		\paragraph{Exemple \ref{ex_clustering}, suite.} 
		\textit{Si on attribut la valeur numérique 0 au $1^{\mathrm{er}}$ avril de l'année $m$, alors on a $T_0^{(m)} = 0,\ T_1^{(m)} = T_0^{(m)} = 0,\ T_2^{(m)}=7$. On a également $D_1^{(m)} = \mathrm{card}(C_1^{(m)}) = 1,\ D_2^{(m)} = \mathrm{card}(C_3^{(m)}) = 2$ et $W_1^{(m)}=T_1^{(0)}=0, W_2^{(m)} = T_{2}^{(m)} - T_{1}^{(m)} - D_{1}^{(m)} = 7 - 0 - 1 = 6$.
		}\\
		
		Soit $\boldsymbol{N}^{(m)}=\{N^{(m)}_s(t),\ s,t\geq 0\} = \{N^{(m)}(s,\ s+t),\ s,t\geq 0\}$, un processus de renouvellement non-stationnaire alterné où un accroissement $N^{(m)}_s(t)$ permet de modéliser le nombre d'événements extrêmes ($\mathrm{card}(\mathcal{X})$) survenus lors de l'intervalle de temps $[s,s+t]$. Dans le contexte des changement climatiques, aucune hypothèse n'est faite sur la stationnarité du processus. Par définition, on a
		\begin{equation}\label{processus_renouvellement}
			N^{(m)}_{T_0}(t) := \sum_{i=1}^{\infty} \mathds{1} \{T_i^{\star (m)} \leq T_0^{(m)} + t\} = \sum_{i=1}^{\infty} \mathds{1} \{T_i^{(m)} + D_i^{(m)} \leq T_0^{(m)} + t\} = \inf\{i: T_i^{(m)} + D_i^{(m)} \leq T_0^{(m)} + t\}.
		\end{equation}
		
		Soit $\boldsymbol{V}^{(m)}=\{V^{(m)}_s(t),\ s,t\geq 0\} = \{V^{(m)}(s,\ s+t),\ s,t\geq 0\}$, un processus de renouvellement non-stationnaire alterné avec récompenses où un accroissement $V^{(m)}_s(t)$ permet de modéliser la quantité totale d'eau accumulée lors des événements extrêmes sur un intervalle de temps $[s,s+t]$. Ainsi, on a 
		\begin{equation}
			V^{(m)}_{T_0}(t) := \sum_{i=1}^{N^{(m)}_{T_0}(t)} X_i^{(m)} = \sum_{i=1}^{\infty} X_i^{(m)}\, \mathds{1}\{T_i^{(m)} + D_i^{(m)} \leq T_0^{(m)} + t\}.
		\end{equation}
		
		Les processus de renouvellement alternés de même que leur homologue avec récompenses sont utilisés dans la littérature en hydrologie, notamment dans \cite{salvadori2006statistical}, \cite{salvadori2007use} et dans \cite{small1986relationship}.\\
		
		Soit $Z^{(m)}$, la v.a. de la quantité de pluie non-extrême totale tombée lors de la saison printanière de l'année $m$. On a $Z^{(m)} := \sum_{j \in \mathcal{Z}^{(m)}} K_j^{(m)}$. La suite de v.a. $\underline{Z} =\{Z^{m}, m\in\natural\}$ n'est pas présumée stationnaire étant donné le contexte des changements climatiques.
		%
		La quantité de pluie totale tombée au cours du printemps d'une année $m$ est donc modélisée avec.
		\begin{equation}\label{Processus_aggrege}
			S^{(m)}_{T_0}(91) = Z^{(m)} + V_{T_0}^{(m)}(91).
		\end{equation}
		
		\paragraph{Exemple \ref{ex_clustering}, suite.} 
		\textit{Pour revenir à l'exemple \ref{ex_clustering}, on a $N_{0}^{(m)}(10) = \mathrm{card}(\mathcal{X}) = 2$ et $V_{0}^{(m)}(10) = X_1^{(m)} + X_2^{(m)} = 30+21 = 51$. Puis, $Z^{m} = K_2^{(m)} = 6$. Finalement $S_0^{(m)}(10) = 51 + 6 = 57.$
		}\\
		
		Pour un exemple plus complet utilisant des données réelles provenant de la rivière Clearwater en Alberta et contenant plus d'une année, voir l'annexe \ref{ex_construction_model}.
		
		
		\subsubsection{Hypothèses du modèle}
		En ce qui concerne l'hypothèse d'indépendance séquentielle, nous considérons qu'il est raisonnable de présumer que $Y_1^{(m)}$ est indépendante de $Y_{91}^{(m-1)}$. De ce fait, il faudrait réaliser le test d'indépendance proposé par \cite{genest2004test} sur chacune des années de façon indépendante. Cependant, comme la méthode proposée réduit grandement le nombre d'observations disponibles pour chacune des années, on se retrouverait avec au plus une vingtaine d'observations par année pour les variables résultantes de l'agrégation. Conséquemment, les résultats du test sont instables et peu concluants. Pour cette raison et par souci de simplicité, l'hypothèse d'indépendance séquentielle est supposée pour chacune des v.a. du modèle.\\
		
		Pour ce qui est de l'hypothèse de stationnarité, considérant le contexte des changements climatiques, aucune présomption n'est faite à ce sujet. Un test de Mann-Kendall est effectué sur chacune des v.a. afin de vérifier l'hypothèse. De plus, lors de la sélection des distributions marginales, l'AIC est calculée autant pour une version stationnaire de chaque loi, que pour une version non-stationnaire. Cette approche est suggérée dans \cite{chebana2021multivariate}.
		
		
		\subsection{Méthodologie}\label{sect_methodologie}
		Afin de pouvoir faire l'étude du processus \eqref{Processus_aggrege}, la méthodologie consiste à paramétrer les distributions marginales en premier. Puis, à l'aide des fonction de répartition théoriques obtenus, d'identifier la structure de dépendance unissant ces v.a. pour choisir la(les) copule(s) appropriée(s). Une fois toute ces composantes obtenues, il est possible d'étudier le comportement de \eqref{Processus_aggrege} par simulation en faisant appel à l'algorithme \ref{algo_Simul_process_St} présenté en annexe.
		
		
		\subsubsection{Distributions marginales}\label{sect_distribution_marginales}
		Avec la théorie des valeurs extrêmes, comme on cherche à modéliser l'ensemble des précipitations qui sortent des normales saisonnières, la méthode POT est tout indiquée pour modéliser les excédents de seuil (voir p.ex. \cite{hosking1987parameter}, \cite{klugman2013loss}). Alors on pose $(X_i-u|X_i\geq u)\sim \mathrm{GPD}(\xi, \sigma)$ telle que la fonction de répartition s'exprime comme
		\begin{equation}\label{cdf_GPD}
			F_u(x) := \begin{cases}
				1-(1+\xi x/\sigma)^{-1/\xi}, & \xi \neq 0,\\
				1-\exp(-x/\sigma), & \xi = 0,
			\end{cases}
		\end{equation}
		où $x\geq0$, $\sigma>0$, $\xi \in\reel$ et $1+\xi x/\sigma > 0$, pour un seuil $u>0$.
		À noter que la paramétrisation de la loi est grandement influencée par la valeur attribuée au paramètre $u$, lequel peut être obtenu par optimisation tel que présenté dans \cite{bader2016automated} et \cite{bader2018automated} ou encore avec \cite{northrop2015cross}. 
		%
		Pour faire suite à \cite{jalbert2019modelling} et pour des fins de simplicité considérant le temps imparti pour ce stage, le seuil utilisé est fixe. Cependant, il est d'intérêt de mentionner que plusieurs auteurs tels que \cite{kysely2010estimating}, \cite{begueria2011assessing} et \cite{cheng2014non} préconisent pour un seuil non-stationnaire. Cette approche sera considérée dans une prochaine version de ce travail. Dans le cas stationnaire, la loi peut être paramétrée selon l'approche des moments ou celles des moments pondérés dépendamment de la forme que prend la loi (voir \cite{hosking1987parameter}). Cependant, dans le cas non-stationnaire, cela est impossible. la méthode utilisée devient alors celle du maximum de vraisemblance généralisée présentée dans \cite{el2007generalized}. Comme cette approche est un dérivé de la paramétrisation bayésienne, l'algorithme MCMC (Simulation Monte-Carlo par chaînes de Markov) est utilisé et les paramètres de la loi doivent avoir des distributions \textit{a priori}.\\
		
		En ce qui concerne à la modélisation de la durée des périodes de pluie et les temps inter-occurrences, considérant que l'échelle de temps utilisée est en jours, les lois envisagées sont discrètes. De plus, considérant que le domaine des v.a. en jeux n'est pas fermé, la loi binomiale est écartée. 
		Conséquemment, les lois retenues sont la loi binomiale négative, son homologue à un paramètre, la loi géométrique et la loi de Poisson. 
		Également, étant donnée sa grande flexibilité, la loi Weibull discrétisée est également retenue.
		Finalement, dans le cas où la queue de la distribution serait très lourde, alors une version discrétisée de la loi GP est envisagée.
		
		Afin de paramétrer la distribution de $W$, on pose la fonction de vraisemblance \eqref{vraisemblance_W}.
		\begin{equation}\label{vraisemblance_W}
			L(\boldsymbol{\theta}|\boldsymbol{w}, \boldsymbol{t}^{\star}) = 
			\prod_{k}
			f_W(w_k|\boldsymbol{\theta}, t_k^{\star}),
		\end{equation}
		où $\boldsymbol{\theta}$ correspond au vecteur des paramètres de la loi de $W$, $\boldsymbol{t}^{\star} = \{t_k^{\star}, k\in\{1,\dots, \card(\boldsymbol{t})\}\}$ correspond à un vecteur d'observations où $t_k^{\star}$ est la $k$-ème observation de $T^{\star}$ dans les données disponibles.
		%
		Puisque l'accroissement du processus de renouvellement est définit sur un intervalle de temps fini, il faut ajouter un terme à \eqref{vraisemblance_W} pour tenir compte du temps qu'il reste à la fin du processus, pour chacune des années. Soit $w_{\mathrm{res}}^{(m)} = 91 - t_{N}^{\star(m)}$ où $t_N^{\star(m)}$ correspond au temps de fin du dernier événement de l'année $m$ et $w_{\mathrm{res}}^{(m)}$ est le temps résiduel du processus. Alors \eqref{vraisemblance_W} devient
		\begin{equation}\label{vraisemblance_W_2}
			L'(\boldsymbol{\theta}|\boldsymbol{w}, \boldsymbol{t}^{\star}) = L(\boldsymbol{\theta}|\boldsymbol{w}, \boldsymbol{t}^{\star}) \times \prod_{m} (1-F_W(w_{\mathrm{res}}^{(m)}|\boldsymbol{\theta}, t_{N}^{\star(m)})).
		\end{equation}
		Dans le cas où $f_W$ est la fonction de densité d'une loi GP, la méthode du maximum de vraisemblance telle que présentée dans \cite{hogg2005introduction} ne peut être utilisée. Il faut plutôt utiliser la méthode du maximum de vraisemblance généralisé présenté dans \cite{el2007generalized}, laquelle nécessite l'algorithme MCMC pour réaliser l'optimisation.\\
		
		Du point de vue des précipitations non-extrêmes, \cite{jalbert2019modelling} recommande l'utilisation la la loi normale. Cependant, comme cette loi admet des valeurs négatives, on considérera aussi la loi gamma comme alternative.
		
		
		\subsubsection{Modélisation de la dépendance}\label{sect_dependance}
		La dépendance unissant les v.a. en jeu est modélisée avec la théorie des copules décrite dans \cite{joe1997multivariate} et \cite{nelsen2006introduction}.
		%
		Dans le contexte particulier des précipitations, on s'intéressera particulièrement à \cite{zhang2019copulas} qui utilise les copules archimédiennes et elliptiques pour modéliser la dépendance entre la durée et l'intensité des précipitations. On s'intéresse également à \cite{salvadori2006statistical} qui défini un processus de renouvellement alterné avec structure de dépendance unissant les variables de durée des épisodes secs, de durée des périodes humides et d'une variable d'intensité pour modéliser les orages. Cette dépendance est modélisée à l'aide d'une copule en vigne tri-variée. 
		
		Dans notre cas, nous reprenons l'idée d'utiliser une copule en vigne pour modéliser la dépendance entre les v.a. de la sévérité $X$, de la durée $D$ et des temps inter-occurrences $W$. La structure de vigne ainsi utilisée est présentée dans l'illustration \ref{structure_CVine}.
		\begin{figure}[H]
			\centering
			\includegraphics[height=0.15\textheight]{graphiques/structure_CVine}
			\caption{Structure C-Vine de la copule tri-variée modélisant la dépendance entre (1) $u^{(X)}$, (2) $u^{(W)}$ et (3) $u^{(D)}$.}
			\label{structure_CVine}
		\end{figure}
		
		Dans le contexte des valeurs extrêmes, on s'intéresse à \cite{gudendorf2010extreme} qui recommande, entre-autre, les copules de Gumbel, de Tawn et de Galambos lorsque les marginales impliquées sont de lois extrêmes.
		%
		De plus, comme le suggère \cite{zhang2019copulas}, les copules archimédiennes sont aussi prises en compte. Parmi celles-ci, on retrouve les copules de Frank, de Clayton ainsi que les copules BB1-BB3 et BB6-BB7 décrites dans \cite{joe1997multivariate}. Finalement, on considère également des rotations à $180\degres$ de ces copules; dans ce cas, on parle alors de copules de survie.
		%
		Les copules elliptiques sont appréciées dans la littérature en hydrologie étant donnée leur flexibilité et la facilité avec laquelle on peut les paramétrer. Cependant, dans le contexte de la modélisation de valeurs extrêmes, nous les écartons, conformément aux conclusions de \cite{renard2007use}, puisque cette famille de copule tend à sous-estimer la dépendance lorsque les marginales sont constituées à la fois de v.a. extrêmes et de v.a. non-extrêmes.\\
		
		Soit $\boldsymbol{x} = \{(X_{i}^{(m)}-u|X_i^{(m)}\geq u), \forall i,m\}$, la suite des observations où $(X_{i}^{(m)}-u|X_{i}^{(m)}\geq u)$ représente l'excédent de seuil observé pour le $i$-ème événement de l'année $m$. 
		Soit $\boldsymbol{w} = \{W_{i}^{(m)}, \forall i,m\}$, la suite des observations où $W_{i}^{(m)}$ représente le temps écoulé entre les $i$-ème et $(i-1)$-ème événements de l'année $m$.
		Soit $\boldsymbol{d} = \{D_{i}^{(m)}, \forall i,m\}$, la suite des observations où $D_{i}^{(m)}$ représente la durée du $i$-ème événement extrême lors de l'année $m$.
		Soit $\boldsymbol{t} = \{T_{i}^{(m)},\forall i,m\}$, la suite des observations où $T_i^{(m)}$ représente le temps de survenance du $i$-ème événement extrême lors de l'année $m$.
		Soit $u^{(X)} = \P(X-u\leq\boldsymbol{x}|X>u)$, le vecteur des uniformes générés en évaluant la fonction de répartition marginale estimée pour les excédents de seuil.
		Soit $u^{(W)} = \P(W\leq\boldsymbol{w}|\boldsymbol{t})$, le vecteur des uniformes générées en évaluant la fonction de répartition marginale estimée pour les temps inter-occurrences.
		Soit $u^{(D)} = \P(D\leq\boldsymbol{d}|\boldsymbol{t})$, le vecteur des uniformes générés en évaluant la fonction de répartition marginale estimée pour la durée de chacun des événements observés. Afin de calculer les mesures de corrélation et de paramétrer les copules de façon adéquate, il est important de considérer la proposition \ref{propos_copules_et_extremes} et la  remarque \ref{remarque_copules_discretes}.
		\begin{proposition}\label{propos_copules_et_extremes}
			L'utilisation des fonctions de répartitions empiriques $F_n(x) = \mathrm{rank}(x)/(n+1)$ sous-tend l'hypothèse que les observations disponibles sont représentatives des minimums et maximums des distributions réelles des données. Hors, comme la v.a. des excédents de seuil fait partie de la famille des lois à valeurs extrêmes, cette hypothèse doit être rejetée. C'est pourquoi, il est mieux d'utiliser les fonctions de répartitions estimées pour produire les pseudo-observations plutôt que d'utiliser une méthode basée sur les rangs comme le suggère généralement la littérature sur la théorie des copules (voir p.ex. \cite{genest2007everything}). 
		\end{proposition}
		\begin{remarque}\label{remarque_copules_discretes}
			Afin de tenir compte du fait que deux des trois v.a. impliquées ont un support discret, \cite{genest2007primer} suggère de ne pas utiliser la méthode des moments pour paramétrer les copules puisque cela insérerait un biais. La méthode de paramétrisation s'appuie donc sur la méthode de la pseudo-vraisemblance (voir \cite{kim2007comparison}). Comme il a été mentionné dans la remarque \ref{propos_copules_et_extremes}, dans le contexte des variables extrêmes, les pseudo-observations doivent être calculées à partir des distributions marginales théoriques plutôt qu'à partir des rangs.
		\end{remarque} 
		
		
		\subsubsection{Modélisation de la non-stationnarité}\label{sect_nonstationnarite}
		Afin de tenir compte d'une éventuelle tendance dans la distribution des différentes v.a. en jeux, on considère que seule la moyenne varie dans le temps et que la variance est stable dans le temps. Cette hypothèse est appuyée par une analyse graphique des séries temporelles associées à chacune des variables. Ces graphiques ne sont pas inclus dans ce rapport par souci de concision. Ainsi, on pose $\mu(t) = a+bt,\ t\geq 0$, comme étant une fonction linéaire servant à modéliser l'espérance des lois. Puis, en posant le paramètre d'échelle de chacune des lois égale à une fonction de cette moyenne, on arrive à paramétrer les lois en représentant adéquatement la tendance. Si, pour une loi donnée, la fonction de moyenne admet des valeurs négatives, alors une alternative est de considérer plutôt $\ln\mu(t) = a+bt, t\geq 0$. Cette approche s'inspire de celle proposée par \cite{khaliq2006frequency}.
		\begin{exemple}
			Soit la v.a. $W$ telle que $W\sim \mathrm{Geo}(p),\ p\in[0,1],$ avec espérance $\E[W]<\infty$ définie comme $\E[W]= 1/p$. Pour paramétrer la loi de $W$ en tenant compte de la tendance, on pose $p(t) = 1/\mu(t) = (a+bt)^{-1},\ a,b,t\in\reel.$
		\end{exemple}
		\begin{exemple}
			Soit la v.a. $W$ telle que $W\sim \mathrm{Weibull}(\alpha, \beta),\ \alpha,\beta>0,$ avec espérance $\E[W]<\infty$ définie comme $\E[W]= \frac{1}{\beta}\Gamma(1+1/\alpha)$. Pour paramétrer la loi de $W$ en tenant compte de la tendance, on pose $\beta(t) = \mu(t) / \Gamma(1+1/\alpha) = \frac{a+bt}{\Gamma(1+1/\alpha)},\ \alpha>0,\ a,b,t\in\reel.$
		\end{exemple}
		
		
		\subsection{Études de cas}
		Afin d'appliquer le modèle proposé dans la section \ref{sect_model}, deux bases de données sont étudiées. La première fait suite à l'étude réalisée par \cite{jalbert2019modelling} et concerne la ville de Burlington dans le Vermont. La seconde touche la rivière Clearwater en Alberta. Dans les deux cas, l'hypothèse de non-stationnarité est vérifiée pour chacune des v.a. en jeux. Différentes distributions sont testées afin de les modéliser et le critère de l'AIC est utilisé pour sélectionner les loi marginales offrant la meilleure adéquation. Cette approche s'appuie sur \cite{chebana2021multivariate} et sur \cite{khaliq2006frequency}. Par la suite, les copules en vignes sont utilisées afin de modéliser la structure de dépendance unissant les différentes composantes du modèle proposé. La sélection de la copule la plus adéquate s'appuie sur \cite{dissmann2013selecting} et les fonction \texttt{R} de la librairie \texttt{VineCopula}.\\		
		
		Pour la première étude de cas, les données utilisées proviennent du site de la National Oceanic and Atmospheric Administration (NOAA)\footnote{\url{https://www.ncdc.noaa.gov/cdo-web/search?datasetid=GHCND}}.
		%
		La station \texttt{USC00431072} a débuté ses opérations en 1884. Cependant, elle comporte quelques données manquantes. Afin d'interpoler sur celles-ci, l'approche recommandée par \cite{shaw2010hydrology} est de moyenner les observations obtenues sur les stations avoisinantes. De plus, comme cette station a fermé le 3 juin 1943, nous avons compensé, comme indiqué par \cite{jalbert2019modelling}, avec la station de l'aéroport de Burlington, soit la station \texttt{USW00014742}, laquelle a commencé ses opérations le $1^{\mathrm{er}}$ décembre 1940. Les données obtenues comporte de l'information jusqu'en 2020, soit 137 ans ($12\,467$ observations printanières).\\
		
		La rivière Clearwater se trouve en Alberta, au Canada.
		Les données utilisées proviennent de la station \texttt{07CD001} qui est située près de Fort McMurray, aux coordonnées GPS suivantes: (56°41'06" N, 111°15'18" W). L'illustration \ref{Map_ClearwaterRiver} présente l'emplacement de la station. Les données utilisées couvrent la période de 1960 à 2013 (53 ans; 4823 observations printanières). Celles-ci sont disponibles sur le site des Relevés hydrologiques du Canada \footnote{\url{https://eau.ec.gc.ca/search/historical_f.html}}. Aucune donnée manquante n'est recensée dans cette base de données.
		
		
		\subsection{Résultats}\label{sect_resultats}
		Avec la méthodologie décrite dans la section \ref{sect_methodologie}, on trouve que, dans les deux études de cas, la loi de sévérité $X-u|X>u$ peut être considérée comme stationnaire. Avec un seuil $u$ de 26.67mm de pluie ($86.39^\mathrm{e}$ percentile) pour le lac Champlain et de 18.29mm ($88.74^\mathrm{e}$ percentile) pour la rivière Clearwater, les estimateurs de la loi GP obtenus sont $\hat{\xi} = 0.08056533$, $\hat{\sigma} = 15.26847280$ pour le premier et $\hat{\xi} = 0.1317715$, $\hat{\sigma} = 14.2908019$ pour le deuxième. Conséquemment, dans les deux cas, la loi possède un domaine non-fini, ce qui constitue une amélioration à la première tentative de paramétrer un modèle POT réalisé par \cite{jalbert2019modelling}. Également, bien que la méthode de sélection de seuil soit automatisé grâce à l'approche de \cite{bader2018automated}, les deux études de cas démontrent des similitudes dans le seuil optimal de la loi GP et dans le paramètre d'échelle. Du point de vue de l'adéquation, les tests d'Anderson-Darling et de Cramer-Von Mises 
		suggérés par \cite{choulakian2001goodness} offrent des \textit{p-values} de 0.132 et de 0.13 pour la première étude de cas, ainsi que de 0.452 et de 0.725 pour la seconde. La loi GP non stationnaire s'agence donc mieux aux données de la rivière Clearwater qu'à ceux du la Champlain.\\
		
		En ce qui a trait à la modélisation des temps inter-occurrences $W$, en comparant l'AIC des lois testées avec et sans tendance, comme il est expliqué dans les sections \ref{sect_distribution_marginales} et \ref{sect_nonstationnarite}, on trouve que la loi qui s'agence le mieux aux données pour la base de données du lac Champlain est la loi géométrique avec les paramètres $\hat{a}^{(W)} = 32.48488, \hat{b}^{(W)} = -0.000410909$.
		
		Pour ce qui est des données de la rivière Clearwater, comme la distribution empirique de la v.a. $W$ possède une queue très lourde, c'est plutôt la loi GP qui modélise le mieux les temps inter-occurrences. Afin de paramétrer cette loi, on pose $\xi^{(W)}_t = \xi^{(W)}$ ainsi que $\sigma^{(W)}_t =\sigma^{(W)},\ t\geq 0$. La distribution \textit{a priori} de ces estimateurs est alors
		\begin{align}\label{GPD_priors_stat}
			\xi^{(W)} \sim \mathcal{N}(0, 0.25),\quad
			\sigma^{(W)} \sim \Gamma(5, 1).
		\end{align}
		Pour ce qui est du cas non-stationnaire, on pose $\xi^{(W)}_t = \xi^{(W)}$ ainsi que $\ln(\sigma^{(W)}_t) = a^{(W)} + b^{(W)} \times t,\ t\geq 0$. Puis, on pose
		\begin{align}\label{GPD_priors_nonstat}
			\xi^{(W)} \sim \mathcal{N}(0, 0.25), \quad
			a^{(W)} \sim \mathcal{N}(\tilde{a}, 3), \quad
			b^{(W)} \sim \mathcal{N}(0, 1),
		\end{align}
		où $\tilde{a}$ correspond au paramètre initial de l'optimisation. Dans ce cas, ce paramètre initial correspond à la moyenne des données observées, soit 34.76.
		Dans ce cas, on trouve que la distribution GP stationnaire est celle qui représente le mieux les données. Ses paramètres estimés sont alors $\hat{\xi}^{(W)}=-0.5056498$ et $\hat{\sigma}^{(W)} = 58.5460087$. On peut donc voir que, comme le paramètre $\hat{\xi}^{(W)}$ est négatif, la loi GP prend la forme d'une loi Beta inversée qui possède un domaine fini sur l'intervalle $[0, -\xi/\sigma]$ (voir \cite{klugman2013loss}). Cependant, comme les données sont censurées, cela ne pause pas de problème du point de vue conceptuel.
		
		On remarque que, pour la première étude de cas, la loi qui s'agençait le mieux était la loi géométrique qui possède une queue de distribution très fine. Tandis que, pour ce qui est de la seconde, c'est la loi GP qui offre la meilleure adéquation, laquelle possède une queue de distribution très lourde. Ce que l'on peut déduire de cette observation, c'est que la localisation de la station utilisée en Alberta possède un climat plus sec que celui de la rivière Champlain, du fait de la chaîne de montagne séparant la Colombie-Britannique de l'Alberta. Les précipitations extrêmes y sont donc moins fréquentes et surviennent à des intervalles plus distancés.\\
		
		Pour ce qui est de la modélisation de la v.a. de la durée des périodes de précipitation extrême $D$, le critère de l'AIC suggère que la loi Weibull avec tendance est celle qui s'agence le mieux aux données dans les deux scénarios. La paramétrisation alors obtenue est $\hat{\alpha}^{(D)}= 2.104619$, $\hat{a}^{(D)}= 3.94734$, $\hat{b}^{(D)} = 1.746127e-05$, pour la première étude de cas, et $\hat{\alpha}^{(D)}= 2.2442893$, $\hat{a}^{(D)}= 1.0591309$, $\hat{b}^{(D)} = 0.7522425$, pour la seconde. 
		On voit alors que le paramètre de forme des deux distributions est quasiment le même, mais que le phénomène de tendance semble beaucoup plus fort pour la base de données de la rivière Clearwater. Cette observation est cohérente avec le test de Mann-Kendall qui offre une \textit{p-value} de 0.2366 pour le lac Champlain et de 0.0002 pour la rivière Clearwater; ce qui laisse présager que la durée des précipitations extrêmes est peu influencée par les changements climatiques dans la région de Burlington, au Vermont, contrairement à la région de Fort McMurray, en Alberta. Comme cette dernière est située plus au nord, près de l'Alaska, peut-être que la fonte des glaciers peut expliquer en partie ce phénomène.\\
		
		Au niveau des précipitations non-extrêmes $Z$, on trouve qu'il n'y a pas d'évidence contre l'hypothèse de stationnarité selon le test de Mann-Kendall.
		En effet, les \textit{p-value} obtenues pour les deux études de cas correspondent à à 0.5124 et à 0.5357.
		
		Du point de vue de la distribution, pour la première étude de cas, les données observées échouent le test de normalité de Shapiro-Wilk. La loi Gamma, en revanche offre une belle adéquation selon les tests de Kolmogorov-Smirnov et d'Anderson-Darling avec des \textit{p-values} de 0.708 et de 0.817. Les paramètres alors estimés sont $\hat{\alpha}^{(Z)}= 14.47781$ et $\hat{\beta}^{(Z)}= 0.10900$, selon la paramétrisation de la loi gamma où l'espérance est calculée avec $\E[Z] = \alpha^{(Z)} / \beta^{(Z)}$.
		
		Du côté des données de la rivière Clearwater, le test de Shapiro-Wilk offre une \textit{p-value} de 0.53576, laissant présager que la loi normale s'agence bien aux données. Les tests d'Anderson-Darling et de Kolmogorov-Smirnov appuient cette observation avec des \textit{p-values} de 0.8768 et de 0.9708. Les paramètres de la loi estimés sont $\hat{\mu}^{(Z)} = 63.54724$ et $\hat{\sigma}^{(Z)}=19.46997$.\\
		
		Une fois toutes les marginales paramétrées, il reste à modéliser la structure de dépendance unissant les v.a. $W$, $D$ et $X$. 		
		Dans un premier temps, l'analyse de la dépendance passe par le calcule des coefficients de corrélations de Pearson sur les pseudo-observations calculés en tenant compte de la proposition \ref{propos_copules_et_extremes}. Se faisant, on trouve les matrices de corrélation \eqref{rho_Pearson} pour les données du Lac Champlain et \eqref{rho_Pearson_2} pour celles de la rivière Clearwater.
		\begin{equation}\label{rho_Pearson}
			\boldsymbol{\rho}_{P}(u^{(X)}, u^{(W)}, u^{(D)}) =
			\begin{pmatrix}
				1.0000000 & 0.13423607 & 0.36863770 \\ 
				0.1342361 & 1.00000000 & 0.01053063 \\ 
				0.3686377 & 0.01053063 & 1.00000000
			\end{pmatrix}.
		\end{equation}
		\begin{equation}\label{rho_Pearson_2}
			\boldsymbol{\rho}_{P}(u^{(X)}, u^{(W)}, u^{(D)}) =
			\begin{pmatrix}
				1.0000000 & 0.1537633 & 0.4171039 \\ 
				0.1537633 & 1.0000000 & 0.2673687 \\ 
				0.4171039 & 0.2673687 & 1.0000000
			\end{pmatrix}.
		\end{equation}
		On remarque alors que seule la dépendance en $W$ et $D$ pour les données du lac Champlain est faible. 
		Un test de Mantel-Haensel (voir \cite{mantel1963chi}) ne permet pas de rejeter l'hypothèse nulle d'indépendance entre les variables $D$ et $W$ pour ce cas-ci. Cependant, considérant que la dépendance est significative pour les autres paires de v.a., alors on considère tout de même une copule tri-variée pour modéliser la dépendance de cette base de données. 
		
		Afin de visualiser la forme des copules bi-variée constituant la copule en vigne,
		les illustrations \ref{pairs_scatterplots_XWD} et \ref{pairs_scatterplots_XWD_2} permettent de visualiser les copules empiriques à partir des nuages de points pour les pseudo-estimateurs définis dans la section \ref{sect_dependance}. Par ailleurs, il faut garder à l'esprit que les v.a. $W$ et $D$ sont toutes-deux discrètes. Or, comme l'explique \cite{genest2007primer}, dans ce cas, plus d'une copule peut s'agencer aux observations puisque celles-ci perdent leur propriété d'unicité dans ce contexte. Cette observation fait en sorte que la forme erratique de la relation entre $W$ et $D$ dans l'illustration \ref{pairs_scatterplots_XWD} peut être expliqué par ce phénomène. De plus, il faut prendre l'asymétrie des copules empiriques avec un grain de sel dans ce contexte. Malgré tout, il serait intéressant de tester une copule asymétrique et e comparer les résultats obtenus avec ceux présentés dans ce rapport, mais cela est laissé à d'autres étant donné le manque de temps pour le stage.
		
		De manière plus formelle, afin de sélectionner les copules bi-variées, \cite{genest2007everything} recommande de comparer les critères de l'AIC et du BIC. Les tableaux \ref{tbl_estim_copules} et \ref{tbl_estim_copules_2} synthétisent les résultats obtenus pour les copules candidates offrant la meilleure adéquation.
		\begin{table}[h]
			\centering
			\begin{tabular}{llcccc}
				\toprule
				Variables & Copules & $\boldsymbol{\theta}$ & log-vrais. & AIC & BIC \\ 
				\hline
				\multirow{5}{3cm}{$(u^{(X)},u^{(D)})$}
				& Gumbel & 1.29 & 24.77 & -47.55 & -43.72 \\ 
				& Frank  & 2.33 & 24.27 & -46.55 & -42.72 \\ 
				& BB8 & [2.45, 0.75] & \textbf{25.83} & \textbf{-47.65} & -40.00 \\ 
				& \textbf{Clayton (180\textdegree)} & \textbf{0.49} & 24.81 & -47.61 & \textbf{-43.78} \\ 
				& BB1 (180\textdegree) & [0.41,1.05] & 25.29 & -46.58 & -38.92 \\ 
				\midrule
				\multirow{5}{3cm}{$(u^{(X)},u^{(W)}|u^{(D)})$}
				& \textbf{Indépendance} & $\emptyset$ & 0.00 & 0.00 & \textbf{0.00} \\ 
				& Gumbel  & 1.1 & 2.12 & -2.25 & 1.58 \\ 
				& Frank & 0.81 & 2.80 & \textbf{-3.60} & 0.23 \\ 
				& Clayton (180\textdegree) & 0.18 & 2.26 & -2.52 & 1.31 \\ 
				& Tawn type 1 (180\textdegree) & [1.34, 0.20] & \textbf{3.57} & -3.13 & 4.52 \\ 
				\bottomrule
			\end{tabular}
			\caption{Résultats de l'estimation des copules candidates pour la base de données du lac Champlain.}
			\label{tbl_estim_copules}
		\end{table}	
		\begin{table}[h]
			\centering
			\begin{tabular}{llcccc}
				\toprule
				Variables & Copules & $\boldsymbol{\theta}$ & log-vrais. & AIC & BIC \\ 
				\hline
				\multirow{5}{3cm}{$(u^{(X)},u^{(D)})$ }
				& \textbf{Gumbel}  & 1.36 & 11.62 & \textbf{-21.24} & \textbf{-18.63} \\ 
				& BB1 & [0, 1.36] & 11.62 & -19.24 & -14.03 \\ 
				& Clayton (180\textdegree) & 0.62 & 10.99 & -19.98 & -17.38\\ 
				& Tawn type 1 (180\textdegree) & [1.95, 0.48] & \textbf{11.63} & -19.26 & -14.05 \\ 
				\midrule
				\multirow{5}{3cm}{$(u^{(W)},u^{(D)})$}
				& \textbf{Frank}  & 1.55 & 3.57 & \textbf{-5.14} & \textbf{-2.53} \\ 
				& BB8 & 3.89 & 3.64 & -3.29 & 1.93\\ 
				& Clayton (180\textdegree) & 0.31 & 2.81 & -3.63 & -1.02\\ 
				& Tawn type 1 (180\textdegree) & [2.06, 0.07] & \textbf{3.82} & -3.64 & 1.57\\ 
				\midrule
				\multirow{5}{3cm}{$(u^{(X)},u^{(W)}|u^{(D)})$}
				& \textbf{Indépendance} & $\emptyset$ & 0.00 & 0.00 & \textbf{0.00} \\ 
				& Frank & 0.84 & 1.14 & -0.28 & 2.33 \\
				& Joe & 1.17 & 0.90 & 0.21 & 2.81 \\
				& Clayton (180\textdegree) & 0.24 &\textbf{ 1.30} & \textbf{-0.61} & 2.00\\ 
				\bottomrule
			\end{tabular}
			\caption{Résultats de l'estimation des copules candidates pour la base de données de la rivière Clearwater.}
			\label{tbl_estim_copules_2}
		\end{table}
		Pour faire la sélection des copules, le critère choisit est le BIC pour des raisons de simplicité du modèle. Ce faisant, les copules sélectionnées sont celles apparaîssant en gras dans les tableaux \ref{tbl_estim_copules} et \ref{tbl_estim_copules_2}.\\
		
		Du point de vue de la dépendance entre le processus aléatoire $\boldsymbol{V}$ et la v.a. $Z$, bien que le rho de Spearman soit significativement supérieur à zéro pour les deux études de cas, l'hypothèse d'indépendance est tout de même utilisée puisque le contraire ajouterais beaucoup de complexité au modèle. Il faudrait alors trouver des 

		
		
		L'illustration \ref{qqplot_S} présente les graphiques quantiles-quantiles des résultats de simulation obtenus avec les deux études de cas.
		\begin{figure}[H]
			\centering
			\begin{subfigure}[l]{0.48\textwidth}
				\includegraphics[width=\textwidth]{graphiques/LacChamplain/qqplot_St}
				\caption{Lac Champlain}
				\label{qqplot_S_champlain}
			\end{subfigure}
			\begin{subfigure}[r]{0.48\textwidth}
				\includegraphics[width=\textwidth]{graphiques/Clearwater/qqplot_St}
				\caption{Rivière Clearwater}
				\label{qqplot_S_clearwater}
			\end{subfigure}
			\caption[QQ-plots]{Diagrammes quantiles-quantiles de $S_{T_0}^{(m)}(91)$: La ligne bleue présente la droite de tendance des quantiles. Les pointillés présentent l'intervalle de confiance au seuil de 5\% pour cette droite et la ligne rouge est la diagonale d'adéquation parfaite. L'objectif est que la diagonale rouge se situe dans l'intervalle de confiance.}
			\label{qqplot_S}
		\end{figure}
		
		
		\subsection{Apprentissages}\label{sect_apprentissages}
		Ce stage a permis de développer mes aptitudes à trouver de la 
		
		\begin{enumerate}
			\item Copules en vigne
			\item Méthode du maximum de vraisemblance généralisée
			\item Algorithme MCMC et sa variante.
			\item Travailler avec des processus de renouvellement alternés
			\item Travailler dans un contexte de non-stationnarité. En dehors des processus de Poisson Non-homogène, qu'est-ce qu'il est possible de faire en termes de processus de renouvellement non-stationnaires
			\item Comment tester l'indépendance séquentielle des données.
			\item Travailler mes aptitudes en recherche de documentation, de rédaction et de programmation avec le langage R.
		\end{enumerate}
	
	
	
	
	
	
	\section{Évaluation du stage}
		\subsection{Nature du travail}
		Ça a été long de trouver le sujet.
		Une fois le sujet trouvé, les choses se sont mises à filer.
		Par moment, la motivation était difficile à trouver:
		\begin{enumerate}
			\item Recherche de documentation laborieuse
			\item apprentissage de nouvelles techniques telles que l'algorithme MCMC et les copules en vigne ont nécessité du temps.
			\item Sensation d'être seul et de manquer de soutien quand les rencontres étaient trop espacées ou lorsque Fateh n'avait pas le temps de m'aider convenablement.
			Heureusement, Étienne et Hélène étaient là pour m'appuyer et m'aider quand Fateh ne le pouvait pas.
			\item Sensation de fatigue accumulée au courant des 5 dernières années qui me rattrape de temps à autres.
		\end{enumerate}
		La rédaction a été la partie la plus difficile. Particulièrement en matière de documentation.
		
		J'ai travaillé à l'envers. J'ai construit un modèle au meilleur de mes connaissances acquises durant ma formation et avec les articles que j'ai lu jusqu'à ce jour. J'ai construit le modèle selon une approche logique et qui me semblait intuitive. Mais, par la suite, quant il est venu le temps d'appuyer les décisions prises avec la littérature, ça a été difficile de trouver des articles qui appuyaient mes décisions. J'aurais sans-doute mieux fait de lire d'avantage avant de construire le modèle et de commencer la programmation. Mais à ce moment là, je n'avais pas encore d'idée claire d'où je m'en allais et de ce qui devait être fait.
		
		\subsection{Environnement de travail}
		Contexte particulier de la pandémie.
		À domicile.
		Rencontres sur ZOOM ou sur Teams.
		Accès à distance aux ressources de documentation de l'université.
		Évaluation/appréciation ? Qu'est-ce qui aurait pu être différent/amélioré ?
		J'aurais aimé que Fateh s'implique un peu plus dans le projet...surtout considérant qu'il voulait que son nom paraisse sur l'article. 15 minutes par semaine pour discuter de l'avancement et partager des idées, ce n'est pas assez. J'aurais aimé avoir plus d'aide pour naviguer dans la littérature en hydrologie et me faire davantage challenger sur mes idées.
		
		
		\subsection{Préparation théorique à l'université}
		\begin{enumerate}
			\item Cours ACT-7000: Modèles mathématiques en Actuariat:
			\begin{enumerate}
				\item La théorie des valeurs extrêmes
				\item Comment paramétrer ce genre de modèle avec des méthodes basées sur les moments
			\end{enumerate}
			\item ACT-2009: Les processus de renouvellement stationnaires
			\item ACT-3000: Théorie du risque:
			\begin{enumerate}
				\item Processus de renouvellement
				\item Théorie des copules
				\item Méthodes de simulation des processus de renouvellement.
			\end{enumerate}
		
		\end{enumerate}
		
	\section{Conclusion}
	En conclusion, je suis très fiers des résultats obtenus lors de ce stage. Bien que le contexte n'ait pas été idéal avec la pandémie, 
	\begin{enumerate}
		\item Faire un rappel sur le projet, les contributions et les apprentissages.
		\item  Faire un rappel sur les points les plus importants de l'appréciation du stage.
	\end{enumerate}
	
	
	
	\bibliographystyle{apalike}
	\bibliography{BibHydrologie}
	
	\clearpage
	\section{Exemple illustrant le modèle}\label{ex_construction_model}
	Pour illustrer les différentes composantes du modèle décrit dans la section \ref{subsect_description_modele}, le tableau \ref{tbl_exemple_illustrer_model} utilise les 15 premières observations printanières de la base de données Clearwater River, pour 5 années consécutives.
	%
	\begin{table}[H]
		\centering
		\resizebox{\columnwidth}{!}{%
			\begin{tabular}{c|ccccccccccccccc}
				\toprule
				$l$ & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 15 \\ 
				\midrule
				$Y^{(2009)}_l$ & - & - & - & - & - & 0.50 & - & 1.99 & 0.50 & - & 0.50 & 3.48 & 0.50 & - & - \\ 
				$Y^{(2010)}_l$ & 0.49 & - & - & 0.49 & - & - & - & 2.05 & 43.78 & 3.42 & 3.91 & - & - & 0.49 & 0.49 \\ 
				$Y^{(2011)}_l$ & - & 0.77 & 0.13 & - & - & - & - & 0.13 & - & - & 2.57 & 1.16 & - & - & - \\ 
				$Y^{(2012)}_l$ & 0.13 & 1.54 & - & 0.13 & 0.13 & 0.13 & - & - & - & 0.39 & - & 0.13 & 3.47 & 11.97 & 0.26 \\ 
				$Y^{(2013)}_l$ & - & - & - & 0.71 & 0.20 & - & - & 0.20 & - & 1.62 & 2.94 & - & 0.51 & - & 0.20 \\ 
				\bottomrule
		\end{tabular}}
		\caption{Observations de $Y_l^{(m)}$, pour $l=1,\dots,15$ et $m=2009,\dots,2013$, dans la base de données Clearwater River. Cette variable est représentée par la colonne \texttt{Precip. (mm)} et la période couvre du $1^{\mathrm{er}}$ au 15 avril des années 2009 à 2013.}
		\label{tbl_exemple_illustrer_model}
	\end{table}
	
	Avec les observations du tableau \ref{tbl_exemple_illustrer_model}, on  peut réaliser le clustering de manière à trouver les résultats du tableau \ref{tbl_exemple_illustrer_model2}. Puis, si on fixe un seuil $u=1$, on obtient $\mathcal{X}^{(2009)} = \{2,3\},\ \mathcal{X}^{(2010)} = \{3\},\ \mathcal{X}^{(2011)} = \{3\},\
	\mathcal{X}^{(2012)} = \{1,4\},\ \mathcal{X}^{(2013)} = \{3\}.
	$ On trouve alors les résultats des tableaux \ref{tbl_exemple_illustrer_model3}, \ref{tbl_exemple_illustrer_model4} et  \ref{tbl_exemple_illustrer_model5}.
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccccc}
			\toprule
			$j$ & 1 & 2 & 3 & 4 & 5 \\
			\midrule
			$\mathcal{C}^{(2009)}_j$ & 6 & 8,9 & 11,12,13 & - & -\\
			$\mathcal{C}^{(2010)}_j$ & 1 & 4 & 8,9,10,11 &14,15 &- \\
			$\mathcal{C}^{(2011)}_j$ & 2,3 & 8 & 11,12 & - & -\\
			$\mathcal{C}^{(2012)}_j$ & 1,2 & 4,5,6 & 10 & 12,13,14,15 &- \\
			$\mathcal{C}^{(2013)}_j$ & 4,5 & 8 & 10,11 & 13 & 15 \\
			\midrule
			$K^{(2009)}_j$ & 0.50 & 2.49 & 4.48 & - & -\\
			$K^{(2010)}_j$ & 0.49 & 0.49 & 53.2 & 0.98 & -\\
			$K^{(2011)}_j$ & 0.90 & 0.13 & 3.73 & - & - \\
			$K^{(2012)}_j$ & 1.67 & 0.39 & 0.39 & 15.83 & - \\
			$K^{(2013)}_j$ & 0.91 & 0.20 & 4.56 & 0.51 & 0.20\\
			\bottomrule
		\end{tabular}
		\caption{Clusterisation des observations du tableau \ref{tbl_exemple_illustrer_model}.}
		\label{tbl_exemple_illustrer_model2}
	\end{table}
	
	\begin{table}[H]
		\parbox{0.47\linewidth}{
			\centering
			\begin{tabular}{c|ccc}
				\toprule
				$i$ &1 &2 & 3\\
				\hline
				$X^{2009}_i$ & 2.49 & 4.48 & - \\
				$X^{2010}_i$ & 53.2 & - & -\\
				$X^{2011}_i$ & 3.73 & - & -\\
				$X^{2012}_i$ & 1.67 & 15.83 & -\\
				$X^{2013}_i$ & 4.56 & - & -\\
				\midrule
				$D^{2009}_i$ & 2 & 3 & - \\
				$D^{2010}_i$ & 4 & - & - \\
				$D^{2011}_i$ & 2 & - & - \\
				$D^{2012}_i$ & 2 & 4 & - \\
				$D^{2013}_i$ & 2 & - & - \\
				\midrule
				$W^{2009}_i$ & 7 & 1 & $\geq2$ \\
				$W^{2010}_i$ & 7 & $\geq4$ & - \\
				$W^{2011}_i$ & 10 & $\geq3$ & - \\
				$W^{2012}_i$ & 0 & 9 & - \\
				$W^{2013}_i$ & 9 & $\geq4$ & - \\
				\bottomrule
			\end{tabular}
			\caption{Construction des v.a. relatives aux événements excédents le seuil $u=1$, à partir du tableau \ref{tbl_exemple_illustrer_model2}.}
			\label{tbl_exemple_illustrer_model3}
			%
		} \hfill \parbox{0.47\linewidth}{
			%
			\centering
			\begin{tabular}{c|ccc}
				\toprule
				\multicolumn{4}{l}{Format date} \\
				$i$ & 0 & 1 & 2 \\
				\hline
				$T^{(2009)}_i$ & 2009-04-01 & 2009-04-08 & 2009-04-11\\
				$T^{(2010)}_i$ & 2010-04-01 & 2010-04-08 & - \\
				$T^{(2011)}_i$ & 2011-04-01 & 2011-04-11 & - \\
				$T^{(2012)}_i$ & 2012-04-01 & 2012-04-01 & 2012-04-12 \\
				$T^{(2013)}_i$ & 2013-04-01 & 2013-04-10 & - \\
				\midrule
				\multicolumn{4}{l}{Format numérique} \\
				$i$ & 0 & 1 & 2 \\
				\hline
				$T^{(2009)}_i$ & 0 & 7 & 10\\
				$T^{(2010)}_i$ & 365 & 372 & - \\
				$T^{(2011)}_i$ & 730 & 740 & - \\
				$T^{(2012)}_i$ & 1096 & 1096 & 1107 \\
				$T^{(2013)}_i$ & 1461 & 1470 & - \\
				\midrule
			\end{tabular}
			\caption{Temps d'arrivée des événements décrits dans le tableau \ref{tbl_exemple_illustrer_model3}.}
			\label{tbl_exemple_illustrer_model4}
		}
	\end{table}
	
	Avec les tableaux \ref{tbl_exemple_illustrer_model2} et \ref{tbl_exemple_illustrer_model3}, on peut calculer les résultats présentés dans le tableau \ref{tbl_exemple_illustrer_model5}.
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccccc}
			\toprule
			$m$ & 2009 & 2010 & 2011 & 2012 & 2013 ­\\
			\hline
			$N^{(m)}_{T_0}(15)$ & 2 & 1 & 1 & 2 & 1 \\
			$V^{(m)}_{T_0}(15)$ & 6.97 & 53.2 & 3.73 & 17.5 & 4.56 \\
			$Z^{(m)}$ & 0.5 & 1.96 & 1.03 & 0.78 & 1.82 \\
			\hline
			$S^{(m)}_{T_0}(15)$ & 7.47 & 55.16 & 4.76 & 18.28 & 6.38\\
			\bottomrule 
		\end{tabular}
		\caption{Représentation de la quantité de pluie totale tombée pendant les 15 premiers jours du printemps des années 2009 à 2013 selon la notation présentée dans la section \ref{sect_model}.}
		\label{tbl_exemple_illustrer_model5}
	\end{table}
	
		
	\clearpage
	\appendix
	\section{Illustrations}
	\begin{figure}[H]
		\centering
		\includegraphics[height=0.35\textheight]{graphiques/LacChamplain/Map_Burlington}
		\caption{Emplacement des stations (1) \texttt{USC00431072}  et (2) \texttt{USW00014742} pour les données de Burlington, au Vermont, USA. {\footnotesize (Image tirée de Google Map)}}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[height=0.35\textheight]{graphiques/Clearwater/Map_ClearwaterRiver}
		\caption{Emplacement de la station \texttt{07CD001} où ont été collectées les données pour l'exemple de la rivière Clearwater. Les coordonnées de la stations sont Longitude -111.2554 et Latitude 56.68528. {\footnotesize (Image tirée de Google Map)}}
		\label{Map_ClearwaterRiver}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[height=0.38\textheight]{graphiques/LacChamplain/pairs_scatterplots_XWD}
		\caption{Copules bi-variées empiriques pour les données du lac Champlain.
			Triangle supérieur de la matrice: Nuage de points des paires de v.a. Triangle inférieur de la matrice: graphiques de contours pour les estimations des  copules empiriques selon une approche par noyau gaussien.}
		\label{pairs_scatterplots_XWD}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[height=0.38\textheight]{graphiques/Clearwater/pairs_scatterplots_XWD}
		\caption{Copules bi-variées empiriques pour les données de la rivière Clearwater.
			Triangle supérieur de la matrice: Nuage de points des paires de v.a. Triangle inférieur de la matrice: graphiques de contours pour les estimations des  copules empiriques selon une approche par noyau gaussien.}
		\label{pairs_scatterplots_XWD_2}
	\end{figure}

	\section{Algorithme de simulation}\label{annexe_algo_simul}
	Soit $F_{X-u|X>u}$, $F_W$ et $F_D$, les fonctions de répartition théoriques des v.a. $\{X-u|X>u\}$, $W$ et $D$. Soit $F^{-1}_{X-u|X>u}$, $F^{-1}_W$ et $^{-1}F_D$, les fonctions quantiles de ces mêmes v.a. La procédure de simulation permettant de générer des réalisations de $S{(m)}(91)$ est décrit dans l'algorithme \ref{algo_Simul_process_St}.\\
	
	\SetKwRepeat{Do}{do}{while}
	\resizebox{!}{0.435\textheight}{
		\begin{algorithm}[H]
			\DontPrintSemicolon
			\SetAlgoLined
			\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
			\Input{
				$m$ (entiers): vecteur des années pour lesquelles on désire simuler;\\
				$t$ (entier): la durée du processus (91 jours dans notre cas);\\
				$N_{\mathrm{max}}$ (entier): Le nombre maximum d'événements attendu dans une année (p. ex. 100);\\
				$n$ (entier): nb de réalisations de la simulation (p. ex. $n=10^3$).
			}
			\Output{matrice de dimension $n\times \mathrm{card}(m)$ correspondant à des réalisations du processus $S^{(m)}_{T_0}(t)$.}
			
			\BlankLine
			\ForEach{m}{
				poser $T_{0} =$ \texttt{as.numeric}(\texttt{as.Date("$m$-04-01")});\;
				poser $t_{\max} = T_{0} + t$;\; 
				\BlankLine
				\For{$j\gets1$ \KwTo $n$}{
					\BlankLine
					simuler $(u_{l}^{(X)}, u_{l}^{(W)}, u_{l}^{(D)}),\ l=1,\dots N_{\mathrm{max}}$, des réalisations de la copule $G$;\;
					poser $i=0$;\;
					\BlankLine
					\Do{$T_{i} <= t_{\max}$}{
						incrémenter $i=i+1$;\;
						calculer $W_{i} = F_{W}^{-1}(u_{i}^{(W)} | T_{i-1})$, une réalisation de $W^{(m)}$ ;\;
						calculer $T_{i} = T_{i-1} + W_{i}$, une réalisation de $T^{(m)}$;\;
						calculer $D_{i} = F_{D}^{-1}(u_{i}^{(D)} | T_{i})$, une réalisation de $D^{(m)}$ ;\;
						calculer $T_{i} = T_{i} + D_{i}$, une réalisation de $T^{\star (m)}$;\;
					}
					\BlankLine
					calculer $N = i-1$, une réalisation de $N^{(m)}_{T_0}(t)$; \;
					\BlankLine
					\If{$N=0$}{
						poser $V=0$; \;
						\Else{
							\For{$i\gets 1$ \KwTo $N$}{
								calculer $X_{i} = u + F^{-1}_{X-u|X>u}(u_i^{(X)})$, une réalisation de $X^{(m)}$;\;
							}
							calculer $V$ = $\sum_{i=1}^{N} X_i$, une réalisation de $V^{(m)}_{T_0}(t)$;\;
					}}
					simuler $Z$, une réalisation de $Z^{(m)}$;\;
					calculer $S_{j, m} = V + Z$, une réalisation de $S_{T_0}^{(m)}(t)$;\;
				}
			}
			\Return S
			\caption{Simuler un processus de renouvellement alterné avec récompense utilisant les copules pour modéliser la dépendance.}
			\label{algo_Simul_process_St}
		\end{algorithm}	
	}
	
		
	
\end{document}